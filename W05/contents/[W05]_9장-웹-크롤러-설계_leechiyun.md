# 9장. 웹 크롤러 설계

**웹 크롤러**

- 검새 엔진에서 널리 쓰는 기술
- 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 목적
    - 웹 페이지, 이미지, 비디오, pdf, …

- **검색 엔진 인덱싱**
    - 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스 생성

- **웹 아카이빙**
    - 나중에 사용할 목적으로 장기보관하기 위한 웹 정보를 모으는 절차

- **웹 마이닝**
    - 데이터 마이닝을 위한 웹 정보 크롤링

- **웹 모니터링**
    - 저작권이나 상표권이 침해되는 사례를 모음

- 데이터 규모에 따라 웹 크롤러의 규모와 기능도 달라진다.

## 1단계. 문제 이해 및 설계 범위 확정

- **규모 확장성**
    - 웹은 거대하다, **병행성**을 활용하면 보다 효과적으로 웹 크롤링 가능
    - 동시에 작업
- **안전성**
    - 악성 코드, 잘못된 웹 페이지 등 비정상적인 입력, 환경에 대응
- **예절**
    - 크롤러 수집 대상 웹 사이트에 짧은 시간 과도한 요청 X
- **확장성**
    - 새로운 형태의 콘테츠의 대응이 쉬워야한다.
    - ex> 이미지, pdf

- **개략적 규모 측정**
    - 매달 10억 개의 웹 페이지 다운
    - QPS = 10억 / 30/24/3600 = 400 페이지
    - 웹 페이지 크기 평균: 500K
    - 10억 페이지 X 500K = 500TB/월

## 2단계. 개략적 설계안 제시 및 동의 구하기

- **시작 URL 집합**
    - 웹 크롤러가 크롤링을 시작하는 출발첨
        - 도메인을 기준으로 크롤링
        - 주제별로 세분화하고 각각의 다른 시작 URL 집합 설정

- **미수집 URL 저장소**
    - 다운로드 할 URL
        - 다운로드 할 URL을 저장 관리할 컴포넌트 = 미수집 URL 저장소
    - 다운로드 될 URL

- **HTML 다운로더**
    - 웹 페이지를 다운로드 하는 컴포넌트
    - 다운로드 할 페이지 URL → 미수집 URL 저장소에서 꺼낸다.

- **도메인 이름 변환기**
    - 도메인(www.a.com) → IP 주소

- **콘텐츠 파서**
    - 파싱(parsing) + 검증(validation)
    - 검증 과정을 통해 불필요한 URL 제거

- **중복 콘텐츠**
    - 문자열을 전부 비교하는 것은 비효율적
    - 웹 페이지의 해시 값 비교

- **콘텐츠 저장소**
    - **저장소를 구현할 때 쓰이는 기술 Tip**
        - 데이터 유형, 크기, 저장소 접근 빈도, 데이터 유효 기간, …
    - 데이터 양이 많기에 디스크로 관리
    - 자주 사용하는 콘텐츠는 메모리에 두어 접근 지연시간을 줄인다.

- **URL 추출기**
    - 상대 경로 → 절대 경로 변환

- **URL 필터**
    - 특정 콘텐츠 타입 or 파일 확장자를 갖는 URL 필터
    - 오류 발생, 접근 제외 목록을 통한 크롤링 대상 배제

- **이미 방문한 URL**
    - 콘텐츠 저장소, 미수집 URL 저장소에 있는 URL은 패스
    - **블룸필터** 활용!

- **URL 저장소**
    - 이미 방문한 URL 보관 저장소

### 작업 흐름

1. 시작 URL들을 미수집 URL 저장소에 저장
2. HTML 다운로더 미수집 URL 저장소에서 URL 목록을 가져온다.
3. HTML 다운로더는 도메인 변환기를 활용 IP 주소로 접속하여 웹 페이지 다운
4. 콘텐츠 파서는 다운로드한 HTML을 파싱하여 올바른 형식인지 검증
5. 중복 콘텐츠 확인, 해당 페이지가 이미 저장소에 있는지
    - 이미 저장소에 있는 경우, 버린다
    - 저장소에 없는 경우, 콘텐츠 저장소에 저장한 뒤, URL 추출
6. URL 추출기로 해당 HTML 페이지 링크 추출
7. URL 필터 적용
8. 중복 URL 판별
    - 저장소에 있는 URL, 버린다
    - 저장소에 없는 경우, 미수집 URL 저장소로 다시 저장

## 3단계. 상세 설계

- **DFS vs BFS**
    - DFS는 그래프의 깊이(크기)가 클수록 비효율적
    - 기본적으로 너비 탐색인 **BFS** 활용
        - 상당수 페이지가 같은 링크 안에서 다른 URL Path 부분을 검색해 과도한 요청 발생
        - 표준적 BFS는 우선순위가 없기에, 페이지 중요도에 따른 처리 필요

### 미수집 URL 저장소

- 미수집 URL 저장소를 활용하면,
    - 과도한 요청 제거 (예의)
    - URL 사이의 우선순위와 신선도를 구현 가능

- **예의**
    - 과도한 요청(Dos)은 웹 페이지 가용성에 문제를 줌
    - 같은 도메인 요청에 대해 계속 요청하지 않고, 시간간격을 둠
        - 큐 라우터
            - 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할
        - 매핑 테이블
            - 호스트 이름과 큐 사이의 관계를 저장하는 테이블
        - FIFO 큐
            - 같은 호스트에 속한 URL은 언제나 같은 큐
        - 큐 선택기
            - 사용할 큐를 선택해 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달
        - 작업 스레드
            - 전달된 URL을 다운로드 수행
            - 작업들 사이에 일정 지연시간(delay)을 둔다.

- **우선순위**
    - 페이지 랭크, 트래픽 양, 갱신 빈도, …
    - 순위 결정 장치
        - 우선순위를 정하는 컴포넌트(계산)
    - 큐(f1, …, fn)
        - 우선순위 별 다른 큐 배치
    - 큐 선택기
        - 큐에서 처리 URL을 꺼냄
        - 순위가 높은 큐를 더 자주 꺼내도록 프로그램
    - 전면 큐(front)
        - 우선순위 결정 과정을 처리
    - 후면 큐(back)
        - 크롤러가 요청을 자주하지 않도록 보장

- **신선도**
    - 웹 페이지는 수시로 변경(추가, 삭제, 업데이트), 따라서 주기적으로 재수집(recrawl) 필요
    - **전략**
        - 웹 페이지의 변경 이력(update history) 활용
        - 우선순위를 활용, 중요한 페이지 자주 재수집

- **미수집 URL 저장소를 위한 지속성 저장장치**
    - 미수집 URL을 모두 메모리에 보관하는 것은 안전성, 규모 확장성 측면에서 바람직하지 않음
        - 메모리의 수억 개의 다르는 URL을 모두 저장하기란 어렵다
        - 대부분의 URL을 디스크 보관, 메모리 버퍼에 큐를 두는 것으로 IO 비용을 줄인다.
        - 버퍼에 있는 데이터는 주기적으로 디스크에 저장

- **HTML 다운로더**
    - HTTP 프로토콜을 통해 웹 페이지를 내려 받음
    - **Robots.txt (**로봇 제외 프로토콜)
        - 웹 사이트가 크롤러와 소통하는 표준적 방법
        - HTML 다운로더가 소통하는 방법
        - 크롤러가 수집해도 되는 페이지 목록을 알려줌
    - **성능 최적화**
        - 분산 크롤링
            - 크롤링 작업을 여러 서버에 분산
        - 도메인 이름 변환 결과 캐시
            - DNS resolber는 크롤러 성능의 병목 중 하나
                - DNS 요청을 보내고 결과를 받기 전까지 다음 작업 진행 X (동기적 특징)
                - 다른 스레드가 DNS 요청을 해도 Block
            - DNS 조회 결과로 얻어진 도메인 이름, IP 주소 사이의 관계를 캐시에 보관 후 **크론 잡**등을 돌려 주기적으로 갱신
    - **지역성**
        - 크롤링 작업을 수행하는 서버 지역별로 분산
        - 네트워크 시간을 줄여 페이지 다운로드 시간을 줄인다
        - 크롤 서버, 캐시, 큐, 저장소 등 대부분 컴포넌트에 적용 가능
    - **짧은 타임아웃**
        - 응답이 느리거나 아예 응답하지 않는 웹 서버 존재
            - 대기시간이 길어지지 않게 타임아웃을 짧게 설정

- **안전성**
    - 안정 해시
    - 크롤링 상태 및 수집 데이터 저장
        - 장애 복구가 가능하도록, 크롤링 상태와 수집 데이터 지속적 저장장치 기록
    - 예외 처리
        - 대규모 시스템에서 에러는 무조건 일어남
        - 예외 처리를 통해 시스템 안정성을 높인다.
    - 데이터 검증

- **확장성**
    - PNG 다운로더
        - PNG 파일을 다운로드 하는 플러그인 모듈
    - 웹 모니터링
        - 웹 모니터링하여 저작권이나 상표권 침해 막음

- **문제 있는 콘텐츠 감지 및 회피**
    - 중복 콘텐츠
        - 해시나 체크섬을 사용해 중복 데이터 탐지
    - 거미 덫
        - 크롤러가 무한 루프 되도록 설계된 웹 페이지 회피 필요
        - URL 필터
    - 데이터 노이즈
        - 광고나 스크립트 코드, 스팸 URL
        - 마찬가지로 필터 처리

## 4단계. 마무리

- **서버 측 렌더링**
    - 동적으로 관리하는 링크를 위한 해결법
- **원치 않는 페이지 필터링**
    - 실제 페이지를 스팸 처리할 수 있기에 필터링 조건을 조절
- **DB 다중화 및 샤딩**
    - 다중화나 샤딩과 같은 기법을 적용하면, 계층 가용성, 규모 확장성, 안정성 향상
- **수평적 규모 확장성**
    - 서버나 상태 정보를 유지하지 않는 stateless 서버로 만들어, 서버를 자유롭게 확장, 축소.

## Tip

- **블룸 필터**
    
    블룸 필터(Bloom Filter)는 원소가 집합에 속하는지 여부를 검사하는데 사용되는 확률적 자료구조입니다. 공간 효율성이 매우 뛰어나지만, 거짓 양성(false positive)이 발생할 수 있는 특징이 있습니다. 자세히 살펴보겠습니다:
    
    1. 기본 구조:
        - 길이가 m인 비트 배열
        - k개의 서로 다른 해시 함수
    2. 작동 원리:
    a. 삽입 (Insert):
    
    b. 검색 (Lookup):
        - 원소를 k개의 해시 함수에 입력하여 k개의 인덱스를 얻음
        - 해당 인덱스의 비트를 1로 설정
        - 검색할 원소를 k개의 해시 함수에 입력하여 k개의 인덱스를 얻음
        - 모든 해당 인덱스의 비트가 1이면 "원소가 존재할 수 있음"
        - 하나라도 0이면 "원소가 존재하지 않음"
    3. 특징:
        - 공간 효율성: 원소 자체를 저장하지 않고 비트만 사용
        - 빠른 삽입과 조회: O(k) 시간 복잡도
        - 삭제 불가능: 원소 제거 기능 없음
        - 거짓 양성: 실제로 없는 원소를 있다고 판단할 수 있음
        - 거짓 음성 없음: 존재하는 원소를 없다고 판단하지 않음
    4. 수학적 분석:
        - 거짓 양성 확률: (1 - e^(-kn/m))^k
        여기서 n은 삽입된 원소 수, m은 비트 배열 크기, k는 해시 함수 개수
        - 최적 해시 함수 개수: k = (m/n) * ln(2)
    5. 응용 분야:
        - 데이터베이스 시스템: 디스크 접근 전 빠른 존재 여부 확인
        - 네트워크: 캐시 확인, 중복 패킷 감지
        - 웹 크롤러: 이미 방문한 URL 확인
        - 스펠링 체커: 사전에 없는 단어 빠르게 확인
    6. 구현 시 고려사항:
        - 비트 배열 크기 선택: 예상 원소 수와 허용 가능한 오류율 고려
        - 해시 함수 선택: 균일 분포를 가진 고속 해시 함수 사용 (예: MurmurHash)
        - 동시성 처리: 멀티스레드 환경에서의 동시 접근 고려
    7. 변형 및 확장:
        - Counting Bloom Filter: 원소 제거 가능
        - Scalable Bloom Filter: 동적으로 크기 조정 가능
        - Compressed Bloom Filter: 압축을 통한 추가 공간 절약
    8. 한계:
        - 정확한 멤버십 테스트 불가능
        - 원소 개수가 증가할수록 오류율 상승
    
    블룸 필터는 대규모 데이터 처리에서 매우 유용하지만, 정확성이 절대적으로 요구되는 경우에는 부적합할 수 있습니다. 사용 시 이러한 특성을 충분히 고려해야 합니다.
    

- **페이지 랭크**
    
    페이지랭크(PageRank)는 구글의 공동 창업자인 래리 페이지와 세르게이 브린이 개발한 웹페이지 순위 결정 알고리즘입니다. 이 알고리즘은 웹의 링크 구조를 분석하여 각 페이지의 상대적 중요도를 측정합니다. 자세히 살펴보겠습니다:
    
    1. 기본 개념:
        - 웹을 거대한 그래프로 간주
        - 웹페이지는 노드, 하이퍼링크는 엣지로 표현
        - 더 많은 링크를 받은 페이지가 더 중요하다고 가정
    2. 수학적 모델:
        - 랜덤 서퍼 모델: 사용자가 무작위로 링크를 클릭한다고 가정
        - 마르코프 체인: 현재 상태가 이전 상태에만 의존하는 확률 과정
    3. 기본 공식:
    PR(A) = (1-d) + d * (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))
    여기서:
        - PR(A): 페이지 A의 페이지랭크
        - d: 감쇠 계수 (일반적으로 0.85)
        - PR(Ti): 페이지 A로 링크하는 페이지 Ti의 페이지랭크
        - C(Ti): 페이지 Ti에서 나가는 링크의 수
    4. 알고리즘 동작 과정:
    a. 모든 페이지에 초기 페이지랭크 값 할당 (예: 1/N, N은 총 페이지 수)
    b. 반복적으로 각 페이지의 페이지랭크 계산
    c. 수렴할 때까지 또는 지정된 반복 횟수에 도달할 때까지 반복
    5. 주요 특징:
        - 링크의 질 고려: 중요한 페이지의 링크가 더 가치 있음
        - 순환 참조 처리: 페이지 간 상호 링크도 적절히 처리
        - 스케일 독립적: 절대적인 링크 수가 아닌 상대적 중요도 측정
    6. 구현 시 고려사항:
        - 대규모 데이터 처리: 분산 컴퓨팅 기술 활용 (e.g., MapReduce)
        - 희소 행렬 처리: 메모리 효율적인 자료구조 사용
        - 수렴 속도: 가속화 기법 적용 (e.g., 외삽법)
    7. 발전 및 변형:
        - 주제별 페이지랭크: 특정 주제에 대한 중요도 계산
        - 개인화된 페이지랭크: 사용자 선호도 반영
        - 시간 기반 페이지랭크: 시간에 따른 중요도 변화 고려
    8. 한계와 문제점:
        - 링크 스팸에 취약: 인위적인 링크 조작 가능성
        - 새로운 페이지 불리: 링크가 적은 새 페이지는 낮은 순위
        - 계산 비용: 웹의 규모가 커질수록 계산 비용 증가
    9. 페이지랭크 이외의 요소:
        - 콘텐츠 관련성
        - 사용자 행동 데이터
        - 페이지 로딩 속도
        - 모바일 친화성 등
    10. 검색 엔진에서의 활용:
        - 검색 결과 순위 결정의 한 요소로 사용
        - 다른 랭킹 신호들과 결합하여 최종 순위 결정
    
    페이지랭크는 웹의 구조를 활용한 혁신적인 알고리즘이지만, 현대의 검색 엔진은 이를 포함한 수많은 요소를 복합적으로 고려하여 검색 결과를 제공합니다. 구글과 같은 주요 검색 엔진들은 지속적으로 알고리즘을 개선하고 있어, 순수한 형태의 페이지랭크보다 더 복잡하고 정교한 방식으로 웹페이지의 중요도를 평가합니다.