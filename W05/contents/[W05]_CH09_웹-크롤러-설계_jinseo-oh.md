[실시간 웹 크롤링 분산 모니터링 시스템 설계 및 구현](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002433880)

크롤러란? 인터넷을 자동 탐색하여 웹 페이지의 데이터를 수집하는 프로그램

### 크롤러의 사용 예시

- 검색 엔진 인덱싱(search engine indexing)
    - 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스(local index)를 만든다.
    - 로컬 인덱스는 검색 엔진이 빠르고 효율적으로 검색 결과를 제공할 수 있도록 구성한 DB이다.
- 웹 아카이빙(web archiving)
- 웹 마이닝(web mining)
    - 인사이트 도출, SNS 데이터 분석하여 트렌드 파악
- 웹 모니터링(web monitoring)
    - 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다.

# 문제 이해 및 설계 범위 확정

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운받은 웹 페이지에서 URL들을 추출한다.
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

### 좋은 웹 크롤러가 만족시켜야 할 속성

- 규모 확장성
    - **병행성(parallelism)**을 활용하면 보다 효과적으로 웹 크롤링 가능
- 안정성(robustness)
    - 웹은 함정으로 가득하며, 크롤러는 이런 장애에 잘 대응할 수 있어야 한다.
- 예절(politeness)
    - 타겟 웹사이트에 과부하를 주지않아야 함.
- 확장성(extensibility)
    - 새로운 형태 컨텐츠를 지원하기 쉬워야 함.

### 개략적 규모 추정

- QPS=10억=400페이지/초
- 최대 QPS=800
- 웹 페이지의 평균 크기=500k
- 10 페이지 x 500k = 500TB/월

# 개략적 설계안 제시

![image](https://github.com/SystemDesign-Plus-Insights/Book-System-Design-Basic/assets/62508156/a2d1b8f9-2046-4233-98c7-0e460f7ee590)

### 시작 URL 집합

- 크롤링 작업을 시작할 때 초기 URL 목록을 말한다.
- **가능한 많은 링크를 탐색**할 수 있도록 하는 URL을 골라야 한다.
    1. 전체 URL 공간을 작은 부분집합으로 나눔 (지역적인 특색 고려, 나라별)
    2. 주제별로 다른 시작 URL을 사용 (쇼핑, 스포츠, 건강 등등)

### 미수집 URL 저장소

- 대부분 현대 웹 크롤러는 크롤링 상태를 두 가지로 관리한다.
    1. 다운로드할 URL
        - 미수집 URL 저장소(URL frontier)라고 부른다. FIFO라고 생각하면 된다.
    2. 다운로드된 URL

### HTML 다운로더

- 인터넷에서 웹 페이지를 다운로드하는 컴포넌트다.
- 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공한다.

### 도메인 이름 변환기

- 웹 페이지를 다운받기 위해 URL을 IP 주소로 변환

### 콘텐츠 파서

- 웹 페이지를 다운하면 파싱(parsing)과 검증(validation)검증을 거쳐야 한다.
- 크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이 느려지게 될 수 있으므로, 독립된 컴포넌트로 만든다.

### 중복 컨텐츠인가?

- 29% 가량의 웹 페이지 컨텐츠는 중복이다.
- 중복된 컨텐츠를 판별하기 위한 방법은 웹 페이지의 해시 값을 비교하는 것이다.

### 컨텐츠 저장소

- HTML 문서를 보관하는 시스템이다.
- 데이터 양이 너무 많으므로 대부분의 컨텐츠는 디스크에 저장하고, 인기 있는 컨텐츠는 메모리에 둔다.

### URL 추출기

- HTML 페이지를 파싱하여 링크들을 골라내는 역할을 한다.
    
    ![image](https://github.com/SystemDesign-Plus-Insights/Book-System-Design-Basic/assets/62508156/9d4bab74-33d8-4ac7-8192-714b5e20d4ce)

    
- 상대경로는 전부 절대 경로로 변환한다.

### URL 필터

- 특정 컨텐츠 타입이나, 파일 확장자를 갖는 URL, 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 한다.

### 이미 방문한 URL?

- 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있는 자료구조 사용한다.
    - 불룸 필터, 해시 테이블이 사용된다.

### URL 저장소

- 이미 방문한 URL을 보관하는 저장소

# 상세 설계

### DFS vs BFS

웹은 페이지는 노드, URL은 에지인 유향 그래프다. 크롤링 프로세스는 이 유향 그래프를 에지에 따라 탐색하는 과정이다.

보통 BFS를 사용하는데 두 가지 문제점이 있다.

- 네트워크 부하
    - 동일한 도메인 내에서 많은 링크를 다운하게 되는 경향이 있어 병렬적으로 수행하면 과부하 문제가 발생할 수 있다.
- 수집하는데 우선순위를 두지 않음
    - 모든 URL을 순차적으로 크롤링하기 때문에, 비효율적이다.

### 미수집 URL 저장소

미수집 URL 저장소를 활용하면 이 문제(네트워크 부하)를 좀 쉽게 해결할 수 있다. 

- 예의
    
    웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 삼가야 한다. 이는 무례한(impolite)일이며, DoS 공격으로 간주될 수 있다.
    
    예의 바른 크롤러를 만드는 데 있어 지켜야 할 한 가지 원칙은, **동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청**한다는 것이다. 같은 웹 사이트의 페이지를 다운받는 테스크는 시간차를 두고 실행하면 될 것이다.
    
    ![image](https://github.com/SystemDesign-Plus-Insights/Book-System-Design-Basic/assets/62508156/9d0105d1-b456-4754-96d2-d01c0db278b9)
    
    - 큐 라우터(queue router): 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장한다.
    - 매핑 테이블(mapping table): 호스트 이름과 큐 사이 관계를 보관하는 테이블이다.
    - FIFO 큐: 각 호스트 별로 큐를 사용하여 URL을 관리한다.
    - 큐 선택기(queue selector): 큐들을 순회하면서 큐에서 URL을 꺼내서 나온 URL을 지정된 작업 스레드에 전달하는 역할을 한다.
    - 작업 스레드(worker thread): 전달된 URL을 다운로드하는 작업을 수행한다. 순차적으로 처리될 것이며, 작업들 사이에는 delay를 둘 수 있다.
    
    **큐 선택기는 동기적??**
    
- 우선순위
    - URL의 우선순위를 나눌 때는 페이지랭크(pagerank),트래픽 양, 갱신 빈도(update frequency) 등 다양한 척도를 사용할 수 있을 것이다.
        
        ![image](https://github.com/SystemDesign-Plus-Insights/Book-System-Design-Basic/assets/62508156/c4844dae-de6e-4dbc-8d5a-b1518f10cb0b)
        
    - 순위결정장치(prioritizer): URL을 입력으로 받아 우선순위를 계산한다.
    - 큐(f1..fn): 우선순위 별로 큐가 하나씩 할당된다.
    - 큐 선택기: 순위가 높은 큐에서 더 자주 꺼내도록 프로그램되어 있다.

![image](https://github.com/SystemDesign-Plus-Insights/Book-System-Design-Basic/assets/62508156/cd787817-e5e2-4f60-83d1-e64d31786a21)

- 신선도
    
    데이터 신선함을 유지하기 위해 주기적으로 recrawl할 필요가 있다. 모든 URL을 재수집하는 것은 많은 비용이 필요하므로 다음과 같은 전략을 사용한다.
    
    - 웹 페이지의 변경 이력 활용(HTTP response header의 Last-Modified)
    - 우선순위를 활용하여, 중요 페이지는 좀 더 자주 재수집

- 미수집 URL 저장소를 위한 지속성 저장장치
    - hybrid approach:  메모리와 디스크를 함께 사용하여 데이터를 관리한다. URL을 저장할 큐를 메모리에 유지하고, 주기적으로 디스크에 저장한다.

### HTML 다운로더

- Robots.txt
    - 웹사이트 소유자가 크롤러에게 웹사이트의 크롤링 정책을 전달하는 파일
    - 웹사이트 어떤 부분을 크롤링 할 수 있는지, 없는지를 지정한다.
- 성능 최적화
    - 분산 크롤링
        - 크롤링 작업을 여러 서버에 분산하는 방법이다.
    - 도메인 이름 변환 결과 캐시
        - DNS Resolver는 크롤러 성능 병목 중 하나이다. (동기적 특성 때문)
        - 따라서 DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이 관계를 캐시에 보관해두고, 크론 잡 등을 돌려 주기적으로 갱신하도록 하면 성능을 높일 수 있다.
    - 지역성
        - 크롤링 작업을 수행하는 서버를 지역적으로 분산
    - 짧은 타임아웃
        - 최대 얼마나 기다릴지 미리 정함
- 안정성
    - 안정해시: 다운로더 서버들에 부하를 분산할 때 적용
- 크롤링 상태 및 수집 데이터 저장
    - 장애가 발생해도 쉽게 복구할 수 있도록 크롤링 상태와 수집 데이터를 지속적 저장장치에 기록해둔다.
- 예외 처리
    - 예외가 발생해도 전체 시스템 중단 없이 작업을 이어나갈 수 있어야 한다.
- 데이터 검증
    - 시스템 오류 방지

### 확장성

![image](https://github.com/SystemDesign-Plus-Insights/Book-System-Design-Basic/assets/62508156/a3dc39ac-c75d-4fe9-8a6d-c69911a320d9)

새로운 모듈을 쉽게 지원할 수 있도록 설계한다.

### 문제 있는 컨텐츠 감지 및 회피

1. 중복 컨텐츠
    - 해시나 체크섬을 사용하여 죽복 컨텐츠 탐지
2. 거미 덫(spider trap)
    - 크롤러를 무한 루프에 빠드리도록 설계한 웹 페이지다.
    - 덫을 자동으로 피하는 알고리즘을 만들기는 까다롭다. 한 가지 방법은 사람이 수작업으로 확인하고 크롤러 탐색 대상에서 제외하거나 URL 필터 목록에 걸어두는 것이다.
3. 데이터 노이즈
    - 광고, 스크립트 코드, 스팸 URL 등
    - 가능하면 제외해야 한다.

# 마무리

- CSR 크롤링 문제
    
    ![image](https://github.com/SystemDesign-Plus-Insights/Book-System-Design-Basic/assets/62508156/56718de0-2253-4f21-9c04-70776bab9a34)

    
    - CSR의 경우, 렌더링이 완료된 이후의 단계에서 크롤링을 시도해야 한다.
    - Selenium을 활용해, 브라우저를 동작시켜 렌더링된 파일을 크롤링 한다.